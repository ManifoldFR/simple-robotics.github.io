<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Enforcing the consensus between Trajectory Optimization and Policy Learning for precise robot control">
  <meta name="keywords" content="Reinforcement Learning, Trajectory Optimization, Robotics, Policy Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enforcing the consensus between Trajectory Optimization and Policy Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://simple-robotics.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://simple-robotics.github.io/publications/simple-gradients/">
            Simple Gradients
          </a>
          <a class="navbar-item" href="https://simple-robotics.github.io/publications/simple-contact-solver/">
            Contact Simulation
          </a>
          <a class="navbar-item" href="https://quentinll.github.io/projects/diffsim/">
            Differentiable System Identification
          </a>
          <a class="navbar-item" href="https://ieeexplore.ieee.org/document/10494919">
            GJK++: accelerated collision detection
          </a>
          <a class="navbar-item" href="https://lmontaut.github.io/diffcol_rs.github.io/">
            Differentiable Collisions Detection
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enforcing the consensus between Trajectory Optimization and Policy Learning for precise robot control</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://quentinll.github.io">Quentin Le Lidec</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://wjallet.github.io">Wilson Jallet</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.di.ens.fr/~laptev/">Ivan Laptev</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.di.ens.fr/~schmid/">Cordelia Schmid</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jcarpent.github.io">Justin Carpentier</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> INRIA Willow</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/paper/lelidec2022consensus.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2209.09006"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Reinforcement learning (RL) and trajectory optimization (TO) present strong complementary advantages. On one hand, RL approaches are able to learn global control policies directly from data, but generally require large sample sizes to properly converge towards feasible policies. On the other hand, TO methods are able to exploit gradient-based information extracted from simulators to quickly converge towards a locally optimal control trajectory which is only valid within the vicinity of the solution. Over the past decade, several approaches have aimed to adequately combine the two classes of methods in order to obtain the best of both worlds. Following on from this line of research, we propose several improvements on top of these approaches to learn global control policies quicker, notably by leveraging sensitivity information stemming from TO methods via Sobolev learning, and augmented Lagrangian techniques to enforce the consensus between TO and policy learning. We evaluate the benefits of these improvements on various classical tasks in robotics through comparison with existing approaches in the literature.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- BibTeX. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">BibTeX</h2>
        <div class="content has-text-left">
          <pre><code>@inproceedings{le2023enforcing,
            title={Enforcing the consensus between Trajectory Optimization and Policy Learning for precise robot control},
            author={Le Lidec, Quentin and Jallet, Wilson and Laptev, Ivan and Schmid, Cordelia and Carpentier, Justin},
            booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
            pages={946--952},
            year={2023},
            organization={IEEE}
          }
          </code></pre>
        </div>
      </div>
    </div>
    <!--/ BibTeX. -->
  </div>
</section>

</body>
</html> 